{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress at 11:55 pm 2023-04-23"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env Setup\n",
    "\n",
    "This is run from local computer with MS vs code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate packages\n",
    "## to import data\n",
    "import os\n",
    "import pickle\n",
    "from datasets import load_dataset\n",
    "\n",
    "## for data processing\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "## for NLP pre-procssing\n",
    "from transformers import AutoTokenizer #,AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, PegasusConfig\n",
    "from transformers import Trainer, TrainingArguments, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "import nlp\n",
    "\n",
    "import evaluate\n",
    "import torch\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global variables\n",
    "localfolderpath = 'C:/Users/TinaM/Desktop/TMB_File/UTS_AUT_2023/36118_ANLP/AT2'\n",
    "gitfolderpath = 'C:/Users/TinaM/Desktop/TMB_File/UTS_AUT_2023/36118_ANLP/AT2/GitHubFolder/TLDR'\n",
    "rawdata_folder = localfolderpath + '/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to store the downloaded dataset\n",
    "dataset_path_train = rawdata_folder + 'multi_news_train.pkl'\n",
    "dataset_path_test =  rawdata_folder + 'multi_news_test.pkl'\n",
    "dataset_path_validation =  rawdata_folder + 'multi_news_validation.pkl'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import raw data\n",
    "\n",
    "The intial dataset are downloaded to local folder. Below is to read from the downloaded files. Otherwise, can be read in directly with below:\n",
    "\n",
    "train_raw = load_dataset(\"multi_news\",split = =\"train\")\n",
    "\n",
    "test_raw = load_dataset(\"multi_news\",split = =\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func to import the downloaded dataset\n",
    "def load_dataset_from_pickle(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        ds = pickle.load(f)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the downloded dataset\n",
    "train_raw = load_dataset_from_pickle(dataset_path_train)\n",
    "test_raw = load_dataset_from_pickle(dataset_path_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data clean\n",
    "\n",
    "Limited to the computation power on personal computer, only 50% of the train data and test data are used in the training process for T5 small and pegasus-cnn_dailymail. \n",
    "\n",
    "HTML, double spaces and line break are removed from the input text with re.sub(). This minimum approach aim to preserve the integrity of the input message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset data to reduce computational demand\n",
    "subset_perc = 0.5\n",
    "\n",
    "# Subset each dataset in half to 2 sets (train & test), use the train set in the modeling process\n",
    "train_set = train_raw.train_test_split(subset_perc=0.5)\n",
    "test_set = test_raw.train_test_split(subset_perc=0.5)\n",
    "\n",
    "# take the 50% records as dataset to use\n",
    "train_set = train_set['train']\n",
    "test_set = test_set['train']\n",
    "\n",
    "# Use the full dataset\n",
    "# train_set = train_raw\n",
    "# test_set = test_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func to clean the input text col\n",
    "def clean_txt(col_name):\n",
    "    # Replace HTML tags with space\n",
    "    txt_clean = re.sub('<[^>]*>',' ',col_name)\n",
    "\n",
    "    # Replace multiple spaces with a single space, leading and trailing space\n",
    "    txt_clean = re.sub('\\s+',' ',txt_clean).strip()\n",
    "\n",
    "    return txt_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the input/document column of the dataset\n",
    "train_set_cleaned = train_set.map(lambda x:{'document': clean_txt(x['document'])})\n",
    "\n",
    "test_set_cleaned = test_set.map(lambda x:{'document': clean_txt(x['document'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset len\n",
    "print(f'\\\n",
    "     Size of the dataset:\\n \\\n",
    "     The train raw data full set has {len(train_raw)} rows, with {train_raw.shape[1]} columns.\\n \\\n",
    "     The train dataset to use has has {len(train_set)} rows, with {train_set.shape[1]} columns.\\n \\\n",
    "     The test raw data full set has {len(test_raw)} rows, with {test_raw.shape[1]} columns.\\n \\\n",
    "     The train dataset to use has has {len(test_set)} rows, with {test_set.shape[1]} columns.\\n \\\n",
    "      ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample record\n",
    "print(f'Sample from train set before cleaning:')\n",
    "print(train_set[0]['document'])\n",
    "print(f'----------------------------')\n",
    "print(f'Sample from train set after cleaning:')\n",
    "print(train_set_cleaned[0]['document'])\n",
    "print(f'----------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selections\n",
    "\n",
    "## Models to compare\n",
    "\n",
    "All three models are based on abstractive text summarisation methods, and trained on a large amount of web pages, books and articles. All three can be used for NLP tasks like document summarization, question answering, and classification tasks.\n",
    "\n",
    "T5-small is a light weight version of the T5 (short for \"Text-to-Text Transfer Transformer\" ) which was developed by Google with 60 million parameters and \n",
    "\n",
    "(Roberts. A, 2020, 'Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformer', Goggle Research Blog, viewed on 2023-04-23, https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)\n",
    "\n",
    "BART (Bidirectional and Auto-Regressive Transformer) is another pre-trained transformer-based model that has been developed by Facebook AI. The version chosen \"distilbart-cnn-12-6\" has 305 million parameters. \n",
    "\n",
    "Similar with the above BART model, the variance of Pegasus chosen \"google/pegasus-cnn_dailymail\" is trained on the \"cnn_dailymail\" dataset. It has the most parameters of 570 million comparing to the other two methods. \n",
    "\n",
    "Each of the model has its own pros and cons on their performance. \n",
    "\n",
    "T5-Small is known for its ability to quickly adapt to new tasks with limited training data, while BART is known for its strong performance on text summarization tasks. Pegasus can generate more fluent and coherent summaries but may require more computational resources compared to T5-Small or BART.\n",
    "\n",
    "The maximum length of input token is different. T5-small has a maximum input length of 512 tokens. (https://jmlr.org/papers/volume21/20-074/20-074.pdf) The sshleifer/distilbart-cnn-12-6 have a maximum input length of 1024 tokens. (https://stackoverflow.com/questions/74228640/which-huggingface-summarization-models-support-more-than-1024-tokens-which-mode) same as google/pegasus-cnn_dailymail, both of them are trained from the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defind model selection metrics\n",
    "All models will be compared by the same metrics: \"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\". These are standard metrics used for text summarisation tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Pre-processing\n",
    "\n",
    "## Tokenization\n",
    "The subset data, both 'document' and 'summary' are tokenized and truncated, then store in a custom instant \"MultiNewsDataset()\" which created the index id before feed into the model. This is to avoid  the error when the train() try to access the backend encoding using integer indexing. \n",
    "\n",
    "\n",
    "## Tuneable Parameters:\n",
    "Turning parameters are stored in variable training_args. The are set with the purpose to reduce the demand of processing power.\n",
    "The per_device_train_batch_size and per_device_eval_batch_size are set up 4, num_train_epochs is set to 2 due to the limited computation power. \n",
    "Metrics are evaluate at the end of each epoch and used \"rouge1\" score to select best weights when the training is compete for each model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model evaluation metric\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all 3 models \n",
    "models = [\n",
    "    {\n",
    "        \"name\": \"t5-small\",\n",
    "        \"model\": T5ForConditionalGeneration,\n",
    "        \"tokenizer\": T5Tokenizer,\n",
    "        \"config\": T5Config,\n",
    "        # Number of parameters: 60,506,624\n",
    "    },\n",
    "    # {\n",
    "    #     \"name\": \"sshleifer/distilbart-cnn-12-6\",\n",
    "    #     \"model\": BartForConditionalGeneration,\n",
    "    #     \"tokenizer\": BartTokenizer,\n",
    "    #     \"config\": BartConfig,\n",
    "    #     # Number of parameters: 305,510,400\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"google/pegasus-cnn_dailymail\",\n",
    "    #     \"model\": PegasusForConditionalGeneration,\n",
    "    #     \"tokenizer\": PegasusTokenizer,\n",
    "    #     \"config\": PegasusConfig,\n",
    "    #     # Number of parameters: 570,797,056, too big for colab\n",
    "    # },\n",
    "]\n",
    "# not working yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute Rouge scores\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # Use the tokenizer's batch_decode method with the provided encodings\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=labels_str, rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"])\n",
    "\n",
    "    return {key: value.mid.fmeasure * 100 for key, value in rouge_output.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an custom instant to house the tokenized text, \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MultiNewsDataset(Dataset):\n",
    "    def __init__(self, input_encodings, output_encodings):\n",
    "        self.input_encodings = input_encodings\n",
    "        self.output_encodings = output_encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_item = {key: torch.tensor(val[idx]) for key, val in self.input_encodings.items()}\n",
    "        output_item = {key: torch.tensor(val[idx]) for key, val in self.output_encodings.items()}\n",
    "        input_item[\"labels\"] = output_item[\"input_ids\"]\n",
    "        return input_item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate each model\n",
    "for model_info in models:\n",
    "    print(f\"Training and evaluating {model_info['name']}...\")\n",
    "\n",
    "    # Print the current time\n",
    "    now = datetime.datetime.now()\n",
    "    print(f\"Tokenisation of {model_info['name']} started at:\", now)\n",
    "\n",
    "    # Start timing the training\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Import tokenizer and model based on the names\n",
    "    print(f\"Import tokenizer and pre-trained model {model_info['name']}...\")\n",
    "    tokenizer = model_info[\"tokenizer\"].from_pretrained(model_info[\"name\"])\n",
    "    model = model_info[\"model\"].from_pretrained(model_info[\"name\"])\n",
    "    print(f\"Number of parameters: {model.num_parameters():,}\")\n",
    "    \n",
    "   # Tokenized dataset\n",
    "    print(f\"Tokenize the train set for {model_info['name']}...\")\n",
    "    train_encodings = tokenizer(train_set_cleaned['document'], truncation=True, padding=True)\n",
    "    train_summary_encodings = tokenizer(train_set_cleaned['summary'], truncation=True, padding=True)\n",
    "    train_dataset = MultiNewsDataset(train_encodings, train_summary_encodings)\n",
    "\n",
    "    print(f\"Tokenize the test set for {model_info['name']}...\")\n",
    "    test_encodings = tokenizer(test_set_cleaned['document'], truncation=True, padding=True)\n",
    "    test_summary_encodings = tokenizer(test_set_cleaned['summary'], truncation=True, padding=True)\n",
    "    test_dataset = MultiNewsDataset(test_encodings, test_summary_encodings)\n",
    "\n",
    "\n",
    "    # Prepare training arguments to use as 'args' in the training process, to store various hyperparameters and settings required for the training and evaluation process.\n",
    "    print(f\"Set up arguments to use in training for {model_info['name']}...\")\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"{model_info['name']}_results\",\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=2,\n",
    "        save_total_limit=1,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"rouge1\",\n",
    "        greater_is_better=True,\n",
    "        predict_with_generate=True,\n",
    "        save_strategy=\"epoch\", \n",
    "    )\n",
    "\n",
    "    # Set the all arguments used in the training process\n",
    "    print(f\"Set up training parrameters for {model_info['name']}...\")\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Print the current time\n",
    "    model_train_start = datetime.datetime.now()\n",
    "    print(f\"Start of the training with {model_info['name']} at {model_train_start}...\")\n",
    "    trainer.train()\n",
    "    model_train_end = datetime.datetime.now()\n",
    "    print(f\"End of the training with {model_info['name']} at {model_train_end}...\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(f\"Evaluaion results from {model_info['name']}...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Results for {model_info['name']}:\", eval_results)\n",
    "\n",
    "    # End timing the training\n",
    "    end_time = time.time()\n",
    "    # Calculate the total training time\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Training time for {model_info['name']} : {training_time:.2f} seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges\n",
    "\n",
    "\t1. Long training time\n",
    "Although the T5-Small is known for its light weight, it is a heave model from a personal computer with the help of GPU. This model estimate to take 16 hours to trained and tested 50% of the original dataset. \n",
    "\n",
    "Even 20% of the data is too much for Google Colab to process, session crashed in the middle of the training process.\n",
    "\n",
    "The Pegasus model process time unknown\n",
    "\n",
    "\t2. Difficult to tune parameters\n",
    "It is very difficult to adjust parameters as it is challenge to finish one training. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current training status\n",
    "Yet to finish one round of training on each model.\n",
    "\t\n",
    "\t• T5-small\n",
    "\t\t○ 667 out of 11244. this initial 6% took 60 mints, total estimate 16 hours. But hang for 2.25 hr at 6%, actual finish time unknown\n",
    "\t• sshleifer/distilbart-cnn-12-6\n",
    "\t\t○ Too much for Colab, wait until other 2 model has one run, then run this\n",
    "\t• google/pegasus-cnn_dailymail\n",
    "\t\t○ To run"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
